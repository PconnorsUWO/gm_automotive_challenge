{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Detection Model Training and Testing Notebook\n",
       "\n",
       "This notebook demonstrates how to train a simple detection model using PyTorch and then test it on new data. The notebook is divided into several sections: \n",
       "\n",
       "1. **Imports and Data Preparation**: Loading the dataset and creating feature representations for each image.\n",
       "2. **Dataset and Dataloader**: Creating a custom dataset class to handle our data.\n",
       "3. **Model Definition and Training**: Defining the neural network model, training it, and printing the loss per epoch.\n",
       "4. **Testing the Model**: Loading new data, running the model in evaluation mode, and printing out predictions or test loss.\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e4f1c1-b2e7-4c93-a8e5-85ed6b5c0c84",
      "metadata": {},
      "outputs": [],
      "source": [
       "import pandas as pd\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "from torch.utils.data import Dataset, DataLoader"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb3d3f8e-59ed-484e-8e7a-f9e5b108b0ed",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load training data\n",
       "df = pd.read_csv(\n",
       "    \"labels_test_8_bit.txt\",\n",
       "    sep=\" \",\n",
       "    names=[\"image_file\", \"class_id\", \"x_min\", \"x_max\", \"y_min\", \"y_max\"]\n",
       ")\n",
       "\n",
       "# Create a features dictionary for each unique image\n",
       "features_dict = {}\n",
       "for img in df['image_file'].unique():\n",
       "    features_dict[img] = torch.randn(256)  # Replace with your actual feature extraction if available"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2db0c91-370d-4e8b-8a1c-c03f2f6d4070",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define a custom Dataset for detection tasks\n",
       "class DetectionDataset(Dataset):\n",
       "    def __init__(self, df, features_dict):\n",
       "        self.df = df\n",
       "        self.features_dict = features_dict\n",
       "\n",
       "    def __len__(self):\n",
       "        return len(self.df)\n",
       "\n",
       "    def __getitem__(self, idx):\n",
       "        row = self.df.iloc[idx]\n",
       "        image_file = row['image_file']\n",
       "        features = self.features_dict[image_file]\n",
       "        bbox = torch.tensor([row['x_min'], row['x_max'], row['y_min'], row['y_max']], dtype=torch.float)\n",
       "        label = torch.tensor(row['class_id'] - 1, dtype=torch.long)\n",
       "        return features, bbox, label"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "09560b22-f5ad-41d2-92bb-5808a8c07e63",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create the dataset and dataloader for training\n",
       "dataset = DetectionDataset(df, features_dict)\n",
       "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad35ac2-9cf1-453f-9ae0-75e17e60b7a1",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define the baseline detection model\n",
       "class BaselineDetector(nn.Module):\n",
       "    def __init__(self, input_dim=256):\n",
       "        super(BaselineDetector, self).__init__()\n",
       "        self.fc = nn.Sequential(\n",
       "            nn.Linear(input_dim, 128),\n",
       "            nn.ReLU(),\n",
       "            nn.Linear(128, 128),\n",
       "            nn.ReLU()\n",
       "        )\n",
       "        self.bbox_head = nn.Linear(128, 4)\n",
       "        self.class_head = nn.Linear(128, 3)  # Assuming 3 classes\n",
       "\n",
       "    def forward(self, x):\n",
       "        features = self.fc(x)\n",
       "        bbox = self.bbox_head(features)\n",
       "        class_logits = self.class_head(features)\n",
       "        return bbox, class_logits"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e73e3f-f8bd-4e4d-a4c3-4a3bda9e3e16",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Instantiate the model, define optimizer and loss functions\n",
       "model = BaselineDetector(input_dim=256)\n",
       "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
       "criterion_bbox = nn.MSELoss()\n",
       "criterion_cls = nn.CrossEntropyLoss()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9f27f0-c3a5-4a5b-a0bd-0e0c7a9e3c65",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Train the model\n",
       "num_epochs = 10\n",
       "for epoch in range(num_epochs):\n",
       "    total_loss = 0.0\n",
       "    for features, bbox, label in dataloader:\n",
       "        optimizer.zero_grad()\n",
       "        pred_bbox, pred_class = model(features)\n",
       "        loss_bbox = criterion_bbox(pred_bbox, bbox)\n",
       "        loss_cls = criterion_cls(pred_class, label)\n",
       "        loss = loss_bbox + loss_cls\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "        total_loss += loss.item()\n",
       "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Testing the Model on New Data\n",
       "\n",
       "The following cells show how to test the model on new data. If labels are available in the new data, you can compute the test loss; otherwise, you can simply retrieve the model predictions."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc84c55-928d-4c0e-bb9b-24a4d7d11a1c",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load new data for testing\n",
       "new_df = pd.read_csv(\n",
       "    \"labels_new_data.txt\",  # Replace with your new data file\n",
       "    sep=\" \",\n",
       "    names=[\"image_file\", \"class_id\", \"x_min\", \"x_max\", \"y_min\", \"y_max\"]\n",
       ")\n",
       "\n",
       "# Create a new features dictionary for the new images\n",
       "new_features_dict = {}\n",
       "for img in new_df['image_file'].unique():\n",
       "    new_features_dict[img] = torch.randn(256)  # Replace with your actual feature extraction\n",
       "\n",
       "# Create the test dataset and dataloader\n",
       "test_dataset = DetectionDataset(new_df, new_features_dict)\n",
       "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9736e5b-5712-4a16-956b-c3a0b5b7e87f",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Option 1: Compute the test loss if labels are available\n",
       "model.eval()\n",
       "test_loss = 0.0\n",
       "with torch.no_grad():\n",
       "    for features, bbox, label in test_loader:\n",
       "        pred_bbox, pred_class = model(features)\n",
       "        loss_bbox = criterion_bbox(pred_bbox, bbox)\n",
       "        loss_cls = criterion_cls(pred_class, label)\n",
       "        loss = loss_bbox + loss_cls\n",
       "        test_loss += loss.item()\n",
       "\n",
       "avg_test_loss = test_loss / len(test_loader)\n",
       "print(f\"Test Loss: {avg_test_loss:.4f}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "c45a2e4e-0d7a-4a60-b60f-cb5c9af1bfc3",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Option 2: Get predictions if labels are not available\n",
       "predictions = []\n",
       "with torch.no_grad():\n",
       "    for features, _, _ in test_loader:\n",
       "        pred_bbox, pred_class = model(features)\n",
       "        predicted_labels = pred_class.argmax(dim=1)\n",
       "        predictions.append((pred_bbox, predicted_labels))\n",
       "\n",
       "if predictions:\n",
       "    sample_pred_bbox, sample_pred_label = predictions[0]\n",
       "    print(\"Sample Predictions:\", sample_pred_bbox, sample_pred_label)\n",
       "else:\n",
       "    print(\"No predictions available.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8d2e1d-1f4f-4b92-9c70-91e1e75c6f97",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Single sample testing\n",
       "model.eval()\n",
       "\n",
       "# Select a single image from the new dataset (here we choose the first one)\n",
       "sample_image = new_df['image_file'].iloc[0]\n",
       "sample_features = new_features_dict[sample_image].unsqueeze(0)  # Add batch dimension\n",
       "\n",
       "with torch.no_grad():\n",
       "    pred_bbox, pred_class = model(sample_features)\n",
       "    predicted_label = pred_class.argmax(dim=1)\n",
       "\n",
       "print(\"Predicted Bounding Box:\", pred_bbox)\n",
       "print(\"Predicted Class:\", predicted_label)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": "3.x"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   